{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WFC3 IR Blob Classification: Data Processing\n",
    "\n",
    "### The purpose of this notebook is to walk the user through the data processing pipeline for the WFC3 IR Blob Classifier. In this notebook, the user will:\n",
    "\n",
    "### 1. Processes IR images. \n",
    "\n",
    "#### The data processing and augmentation pipeline is as follows:\n",
    "#### - Use vmin/vmax from zscale function to determine clip min/max values for data.\n",
    "#### - Change border and nan values to median.\n",
    "#### - Normalize data to N(0, 1), where blobs are positive values, background is near 0, and random noise are negative values.\n",
    "#### - Grab random 256x256 subframes from a 1024x1024 image. The size of the subframe significantly increases computation efficiency.\n",
    "#### - Add random noise (N(0, 0.75)), rotation, and flipping to subframes to diversify data set.\n",
    "\n",
    "### 2. Produces training, validation, and test sets of IR blob subframes.\n",
    "#### With the data processing pipeline, we can generate and save training, validation, and test sets, containinng hundreds to thousands of subframes from 1024x1024 blob images. \n",
    "#### We can even superimpose blobs onto non blob subframes to increase blob data set diversity, which increases the chances that the model truly learns what a blob is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction (Pending...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "### This notebook also uses functions defined in the complimentary python script, wfc3_ir_blob_class_utils.py. Please read the documentation of the script for further knowledge about it's functionality under Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from ginga.util.zscale import zscale\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wfc3_ir_blob_class_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and plot preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOB = 'example_data/example_blob.fits'\n",
    "NON_BLOB = 'example_data/example_non_blob.fits'\n",
    "MEDIAN = 'example_data/example_median_stack.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blob_pre = plot_pre_processed(BLOB, show_hist=False, stack=0)\n",
    "non_blob_pre = plot_pre_processed(NON_BLOB, show_hist=False, stack=0)\n",
    "median_pre = plot_pre_processed(MEDIAN, show_hist=False, stack=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data\n",
    "### The machine learning algorithm cannot take non finite pixel values. They must either be changed to the median or clipped to a min/max value. In addition, normalizing data is a common practice in machine learning to increase computation efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_norm = scale_data(blob_pre)\n",
    "non_blob_norm = scale_data(non_blob_pre)\n",
    "median_norm = scale_data(median_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FACTOR = 0.5\n",
    "\n",
    "blob_norm = plot_scaled_blobs(BLOB, stack=0, factor=FACTOR)\n",
    "non_blob_norm = plot_scaled_blobs(NON_BLOB, stack=0, factor=FACTOR)\n",
    "median_norm = plot_scaled_blobs(MEDIAN, stack=1, factor=FACTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut image into 16 unique subframes and plot\n",
    "### The end goal of the model is to classify 16 unique subframes to check if any new blobs appear. We'll save these subframes to be used as a final test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir saved_generated_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_full = grab_full_subframes(blob_norm)\n",
    "non_blob_full = grab_full_subframes(non_blob_norm)\n",
    "median_full = grab_full_subframes(median_norm)\n",
    "np.savez_compressed('saved_generated_datasets/final_test.npz', \n",
    "                    blob=blob_full, \n",
    "                    non_blob=non_blob_full, \n",
    "                    median=median_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,4,figsize=[10,10])\n",
    "blob_full = np.array(blob_full).reshape(4,4,256,256)\n",
    "for i in range(4):\n",
    "    for j in range (4):\n",
    "        axs[i, j].set_title('Unique Frame: {}'.format(i * 4 + j))\n",
    "        axs[i, j].imshow(blob_full[i, j], cmap='Greys')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,4,figsize=[10,10])\n",
    "non_blob_full = np.array(non_blob_full).reshape(4,4,256,256)\n",
    "for i in range(4):\n",
    "    for j in range (4):\n",
    "        axs[i, j].set_title('Unique Frame: {}'.format(i * 4 + j))\n",
    "        axs[i, j].imshow(non_blob_full[i, j], cmap='Greys')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,4,figsize=[10,10])\n",
    "median_full = np.array(median_full).reshape(4,4,256,256)\n",
    "for i in range(4):\n",
    "    for j in range (4):\n",
    "        axs[i, j].set_title('Unique Frame: {}'.format(i * 4 + j))\n",
    "        axs[i, j].imshow(median_full[i, j], cmap='Greys')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building make_dataset()\n",
    "### These next few blocks show the functionality of the data augmentation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab a random subframe from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_random = grab_random_subframe(blob_norm)\n",
    "non_blob_random_train = grab_random_subframe(non_blob_norm, sets=[True, False])\n",
    "median_random_val = grab_random_subframe(median_norm, sets=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip and plot random subframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_flip = flip(blob_random)\n",
    "non_blob_flip = flip(non_blob_random_train)\n",
    "median_flip = flip(median_random_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = [blob_random, non_blob_random_train, median_random_val]\n",
    "flipped = [blob_flip, non_blob_flip, median_flip]\n",
    "random_flipped_subframes = np.array([random, flipped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Random Blob Subframe', 'Random Non Blob Subframe', 'Random Median Stack Subframe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, _, __ = random_flipped_subframes.shape\n",
    "fig, axs = plt.subplots(x, y, figsize=[x*10,y*3])\n",
    "for i in range (x):\n",
    "    for j in range (y):\n",
    "        if i == 1:\n",
    "            titles[j] += ' (Flipped)'\n",
    "        axs[i, j].set_title(titles[j])\n",
    "        axs[i, j].imshow(random_flipped_subframes[i, j], cmap='Greys')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superimpose random blobs on non blob subframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_superimpose = add_blobs(non_blob_random_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=[10,20])\n",
    "axs[0].set_title('Random Non Blob Subframe')\n",
    "axs[0].imshow(non_blob_random_train, cmap='Greys')\n",
    "axs[1].set_title('Random Non Blob Subframe with Superimposed Blobs')\n",
    "axs[1].imshow(blob_superimpose, cmap='Greys')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset from one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = make_dataset(MEDIAN, \n",
    "                        100, \n",
    "                        stack=1, \n",
    "                        factor=0.5, \n",
    "                        superimpose=False, \n",
    "                        sets=[False, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save example generated datasets using superimpose. \n",
    "### CAUTION: In this example, we are creating training, validation, and test sets for bob and non blob data. Each of these six sets have a size of 200 subframes, 256x256 pixels per subframe. Each set is around 50MB of space. Creating substantially larger datasets will use a lot of space (e.g. tens of GB). When creating larger datasets, it's recommended to either:\n",
    "#### 1. Prepare your local device to store the sets\n",
    "#### 2. Save sets on a hard drive\n",
    "#### 3. Save sets on a path with more space (i.e. central storage)\n",
    "### Storage space is user based so prepare in whichever way is most comfortable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non blob training set\n",
    "\n",
    "path = 'saved_generated_datasets/training_non_blob.npz'\n",
    "save_image_set, save_labels = save_generated_dataset([NON_BLOB], \n",
    "                                                     path, \n",
    "                                                     blob=False, \n",
    "                                                     itertimes=100, \n",
    "                                                     stack=0, \n",
    "                                                     factor=0.5, \n",
    "                                                     plot=False, \n",
    "                                                     superimpose=False, \n",
    "                                                     sets=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blob training set\n",
    "\n",
    "path = 'saved_generated_datasets/training_blob.npz'\n",
    "save_image_set, save_labels = save_generated_dataset([NON_BLOB], \n",
    "                                                     path, \n",
    "                                                     blob=True, \n",
    "                                                     itertimes=100, \n",
    "                                                     stack=0, \n",
    "                                                     factor=0.5, \n",
    "                                                     plot=False, \n",
    "                                                     superimpose=True, \n",
    "                                                     sets=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non blob validation set\n",
    "\n",
    "path = 'saved_generated_datasets/validation_non_blob.npz'\n",
    "save_image_set, save_labels = save_generated_dataset([NON_BLOB], \n",
    "                                                     path, \n",
    "                                                     blob=False, \n",
    "                                                     itertimes=100, \n",
    "                                                     stack=0, \n",
    "                                                     factor=0.5, \n",
    "                                                     plot=False, \n",
    "                                                     superimpose=False, \n",
    "                                                     sets=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blob validation set\n",
    "\n",
    "path = 'saved_generated_datasets/validation_blob.npz'\n",
    "save_image_set, save_labels = save_generated_dataset([NON_BLOB], \n",
    "                                                     path, \n",
    "                                                     blob=True, \n",
    "                                                     itertimes=100, \n",
    "                                                     stack=0, \n",
    "                                                     factor=0.5, \n",
    "                                                     plot=False, \n",
    "                                                     superimpose=True, \n",
    "                                                     sets=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non blob test set\n",
    "\n",
    "path = 'saved_generated_datasets/test_non_blob.npz'\n",
    "save_image_set, save_labels = save_generated_dataset([NON_BLOB], \n",
    "                                                     path, \n",
    "                                                     blob=False, \n",
    "                                                     itertimes=100, \n",
    "                                                     stack=0, \n",
    "                                                     factor=0.5, \n",
    "                                                     plot=False, \n",
    "                                                     superimpose=False, \n",
    "                                                     sets=[False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blob test set\n",
    "\n",
    "path = 'saved_generated_datasets/test_blob.npz'\n",
    "save_image_set, save_labels = save_generated_dataset([NON_BLOB], \n",
    "                                                     path, \n",
    "                                                     blob=True, \n",
    "                                                     itertimes=100, \n",
    "                                                     stack=0, \n",
    "                                                     factor=0.5, \n",
    "                                                     plot=False, \n",
    "                                                     superimpose=True, \n",
    "                                                     sets=[False, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete! Move onto Modeling Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
